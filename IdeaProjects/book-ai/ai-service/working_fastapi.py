#!/usr/bin/env python3
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from openai import OpenAI
from enum import Enum
from datetime import datetime
import uuid
import uvicorn
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
OPENAI_CLIENT = OpenAI(api_key="your_openai_api_key_here")

logger.info("OpenAI client initialized successfully")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Book AI Analysis Service",
    description="ì±… ë…ì„œ ê¸°ë¡ AI ë¶„ì„ ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„° ëª¨ë¸
class Genre(str, Enum):
    TECHNICAL = "TECHNICAL"
    LITERATURE = "LITERATURE"

class AnalysisType(str, Enum):
    LITERATURE_ANALYSIS = "LITERATURE_ANALYSIS"
    TECHNICAL_SUMMARY = "TECHNICAL_SUMMARY"

class AnalysisRequest(BaseModel):
    user_id: str = Field(alias="user_id")
    book_id: str = Field(alias="book_id")
    book_title: str = Field(alias="book_title")
    book_author: str = Field(alias="book_author")
    genre: Genre
    reading_content: str = Field(alias="reading_content")
    
    class Config:
        populate_by_name = True

class AnalysisResponse(BaseModel):
    analysis_id: str
    user_id: str
    book_id: str
    analysis_type: AnalysisType
    content: str
    created_at: datetime

class APIResponse(BaseModel):
    success: bool
    data: AnalysisResponse = None
    message: str = None
    error_code: str = None

# í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/v1/health/")
async def health_check():
    return {
        "success": True,
        "data": {
            "status": "UP",
            "timestamp": datetime.now().isoformat(),
            "service": "book-ai-analysis-service"
        },
        "message": "ì„œë¹„ìŠ¤ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤",
        "error_code": None
    }

# AI ë¶„ì„ ìƒì„± ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/v1/analysis/generate", response_model=dict)
async def generate_analysis(request: AnalysisRequest):
    """ë…ì„œ ê¸°ë¡ AI ë¶„ì„ ìƒì„±"""
    try:
        logger.info(f"ë¶„ì„ ìš”ì²­ ìˆ˜ì‹ : ì‚¬ìš©ì {request.user_id}, ì±… {request.book_title}")
        
        # ì¥ë¥´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if request.genre == Genre.LITERATURE:
            prompt = f"""
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ë…ì„œ ê¸°ë¡ì…ë‹ˆë‹¤:

ì±… ì œëª©: {request.book_title}
ì €ì: {request.book_author}
ë…ì„œ ê¸°ë¡: {request.reading_content}

ì´ ë¬¸í•™ ì‘í’ˆì— ëŒ€í•œ ì‹¬ë„ ìˆëŠ” ë¶„ì„ì„ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ“– ë…ì„œ ì—¬ì • ë¶„ì„
- ë…ì„œ ê¸°ë¡ì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ë…ìì˜ ê´€ì ê³¼ ê°ì • ë³€í™”
- ì‘í’ˆì´ ë…ìì—ê²Œ ë¯¸ì¹œ êµ¬ì²´ì ì¸ ì˜í–¥ê³¼ ê¹¨ë‹¬ìŒ
- ê°œì¸ì  ê²½í—˜ê³¼ ì—°ê²°ë˜ëŠ” ë¶€ë¶„ë“¤

## ğŸ­ ì‘í’ˆ í•µì‹¬ ë¶„ì„
- ì£¼ìš” ì£¼ì œì™€ ë©”ì‹œì§€ì˜ í˜„ëŒ€ì  í•´ì„
- ì¸ë¬¼ì˜ ì‹¬ë¦¬ì  ë™ê¸°ì™€ ê°ˆë“± êµ¬ì¡°
- ìƒì§•ê³¼ ì€ìœ ì˜ ì˜ë¯¸ì™€ íš¨ê³¼
- ì„œì‚¬ êµ¬ì¡°ì™€ ë¬¸í•™ì  ê¸°ë²•ì˜ íŠ¹ì§•

## ğŸŒ ë¬¸í™”ì  ë§¥ë½
- ì‘í’ˆì´ ì“°ì¸ ì‹œëŒ€ì  ë°°ê²½ê³¼ ì‚¬íšŒì  ì˜ë¯¸
- ë³´í¸ì  ì¸ê°„ ê²½í—˜ìœ¼ë¡œì„œì˜ ê°€ì¹˜
- í˜„ëŒ€ ë…ìë“¤ì´ ê³µê°í•  ìˆ˜ ìˆëŠ” ì§€ì 

## ğŸ’¡ í†µì°°ê³¼ ì„±ì°°
- ì´ ì‘í’ˆì„ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ” ì¸ìƒì˜ ì§€í˜œ
- ë…ìì˜ ì„¸ê³„ê´€ í™•ì¥ì— ê¸°ì—¬í•˜ëŠ” ë¶€ë¶„
- ì¼ìƒì— ì ìš©í•  ìˆ˜ ìˆëŠ” êµí›ˆ

## ğŸ“š ì—°ê´€ ë…ì„œ ê°€ì´ë“œ
- ë¹„ìŠ·í•œ ì£¼ì œì˜ ì¶”ì²œ ì‘í’ˆ 3ê¶Œ (ì´ìœ ì™€ í•¨ê»˜)
- ì €ìì˜ ë‹¤ë¥¸ ì‘í’ˆ ì¤‘ ì½ì–´ë³¼ ë§Œí•œ ì±…
- ê´€ë ¨ ì¥ë¥´ë‚˜ ë¬¸í•™ì‚¬ì  ë§¥ë½ì˜ ì‘í’ˆë“¤

ê° ì„¹ì…˜ì€ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ë…ìì˜ ì§€ì  í˜¸ê¸°ì‹¬ì„ ìê·¹í•˜ê³  ë” ê¹Šì´ ìˆëŠ” ë…ì„œë¥¼ ìœ ë„í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
            analysis_type = AnalysisType.LITERATURE_ANALYSIS
        else:  # TECHNICAL
            prompt = f"""
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ê¸°ìˆ ì„œì  ë…ì„œ ê¸°ë¡ì…ë‹ˆë‹¤:

ì±… ì œëª©: {request.book_title}
ì €ì: {request.book_author}
ë…ì„œ ê¸°ë¡: {request.reading_content}

ì´ ê¸°ìˆ ì„œì ì— ëŒ€í•œ ì‹¤ë¬´ ì¤‘ì‹¬ì˜ ì¢…í•© ë¶„ì„ì„ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ”‘ í•µì‹¬ ê°œë… ë§ˆìŠ¤í„°ë¦¬
- ë…ì„œ ê¸°ë¡ì—ì„œ ì–¸ê¸‰ëœ í•µì‹¬ ê¸°ìˆ  ê°œë…ë“¤ì˜ ì •í™•í•œ ì´í•´ë„ í‰ê°€
- ê° ê°œë…ì´ ì‹¤ë¬´ì—ì„œ í•´ê²°í•˜ëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œë“¤
- ê¸°ì¡´ ë°©ì‹ ëŒ€ë¹„ ì´ ê¸°ìˆ /ë°©ë²•ë¡ ì˜ ì¥ë‹¨ì 
- í•™ìŠµìê°€ ë†“ì¹˜ê¸° ì‰¬ìš´ ì¤‘ìš”í•œ ë””í…Œì¼ë“¤

## ğŸ’» ì‹¤ë¬´ ì ìš© ì „ëµ
- í˜„ì¬ í”„ë¡œì íŠ¸ì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë°©ë²•
- ë„ì… ì‹œ ì˜ˆìƒë˜ëŠ” ì–´ë ¤ì›€ê³¼ í•´ê²° ë°©ì•ˆ
- íŒ€ ë‹¨ìœ„ ë„ì… ì‹œ ê³ ë ¤ì‚¬í•­ê³¼ ë‹¨ê³„ë³„ ì ‘ê·¼ë²•
- ì„±ê³¼ ì¸¡ì • ë°©ë²•ê³¼ ê°œì„  ì§€í‘œ

## ğŸ› ï¸ ì‹¤ì „ ì½”ë“œ ê°€ì´ë“œ
- ì±…ì˜ ì´ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ
- ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ì™€ ë””ë²„ê¹… íŒ
- ì„±ëŠ¥ ìµœì í™” í¬ì¸íŠ¸
- í™•ì¥ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ ê³ ë ¤í•œ ì„¤ê³„ ì›ì¹™

## ğŸš€ ì»¤ë¦¬ì–´ ì„±ì¥ ë¡œë“œë§µ
- ì´ ê¸°ìˆ  ì˜ì—­ì—ì„œì˜ ì „ë¬¸ì„± ë°œì „ ë°©í–¥
- ê´€ë ¨ ê¸°ìˆ  ìŠ¤íƒê³¼ì˜ ì‹œë„ˆì§€ íš¨ê³¼
- ì—…ê³„ íŠ¸ë Œë“œì™€ ë¯¸ë˜ ì „ë§
- í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´

## ğŸ“– í•™ìŠµ ë¦¬ì†ŒìŠ¤ íë ˆì´ì…˜
- ì‹¬í™” í•™ìŠµì„ ìœ„í•œ ì¶”ì²œ ë„ì„œ 3ê¶Œ (ë‚œì´ë„ë³„)
- ì‹¤ìŠµ ì¤‘ì‹¬ì˜ ì˜¨ë¼ì¸ ì½”ìŠ¤ì™€ ë¬¸ì„œ
- ì»¤ë®¤ë‹ˆí‹°ì™€ ì‹¤ë¬´ì§„ë“¤ì´ í™œìš©í•˜ëŠ” ë ˆí¼ëŸ°ìŠ¤
- ìµœì‹  ì—…ë°ì´íŠ¸ì™€ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤

ê° ì„¹ì…˜ì€ ì¦‰ì‹œ ì‹¤ë¬´ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ì‹¤ìš©ì  ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±í•˜ë˜, ê°œë°œìì˜ ê¸°ìˆ ì  ì„±ì¥ê³¼ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ í–¥ìƒì— ì§ì ‘ ë„ì›€ì´ ë˜ë„ë¡ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
            analysis_type = AnalysisType.TECHNICAL_SUMMARY
        
        # OpenAI API í˜¸ì¶œ
        logger.info("OpenAI API í˜¸ì¶œ ì‹œì‘")
        response = OPENAI_CLIENT.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì±… ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë…ì„œ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.7
        )
        
        analysis_content = response.choices[0].message.content.strip()
        logger.info(f"OpenAI API í˜¸ì¶œ ì„±ê³µ, ë¶„ì„ ê¸¸ì´: {len(analysis_content)}")
        
        # ë¶„ì„ ê²°ê³¼ ì‘ë‹µ ìƒì„±
        analysis_response = AnalysisResponse(
            analysis_id=str(uuid.uuid4()),
            user_id=request.user_id,
            book_id=request.book_id,
            analysis_type=analysis_type,
            content=analysis_content,
            created_at=datetime.now()
        )
        
        logger.info(f"ë¶„ì„ ì™„ë£Œ: {analysis_response.analysis_id}")
        
        return {
            "success": True,
            "data": analysis_response.dict(),
            "message": "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "error_code": None
        }
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    return {
        "service": "Book AI Analysis Service",
        "version": "1.0.0",
        "status": "running",
        "port": 8000
    }

if __name__ == "__main__":
    logger.info("FastAPI ì„œë²„ ì‹œì‘ - í¬íŠ¸ 8000")
    uvicorn.run(app, host="0.0.0.0", port=8000)